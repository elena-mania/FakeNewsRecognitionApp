{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Crearea setului de date\n",
        "1. Preluam stirile de pe veridica\n",
        "2. Verificam similaritatea dintre ele si cele din setul online\n",
        "3. Preluam stirile de pe DIgi24\n",
        "4. Preluam stirile de pe TNR\n",
        "5. Combinam toate datele intr un singur set"
      ],
      "metadata": {
        "id": "pO_RCE66K2gj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jDF33owKsoS"
      },
      "outputs": [],
      "source": [
        "#preluam baza de date veridica\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import csv\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "BASE_URL = \"https://www.veridica.ro\"\n",
        "START_PAGE = 1\n",
        "END_PAGE = 187\n",
        "articles_data = []\n",
        "MAX_WORKERS = 10  #nr de threaduri\n",
        "\n",
        "def scrape_article_page(article_url):\n",
        "    try:\n",
        "        response = requests.get(article_url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # luam titlul si tag-ul\n",
        "        title_tag = soup.find(\"h1\", class_=\"responsiveTitle\")\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "        title_upper = title.upper()\n",
        "\n",
        "        # verificam daca titlul incepe cu \"Veridica.md:\"\n",
        "        if title_upper.startswith(\"VERIDICA.MD:\"):\n",
        "            if \"FAKE NEWS\" in title_upper:\n",
        "                tag = \"fake_news\"\n",
        "            elif \"PROPAGANDĂ\" in title_upper or \"PROPAGANDA\" in title_upper:\n",
        "                tag = \"propaganda\"\n",
        "            elif \"DEZINFORMARE\" in title_upper:\n",
        "                tag = \"misinformation\"\n",
        "            else:\n",
        "                tag = \"unknown\"\n",
        "        else:\n",
        "            if title_upper.startswith(\"FAKE NEWS\"):\n",
        "                tag = \"fake_news\"\n",
        "            elif title_upper.startswith(\"PROPAGANDĂ\") or title_upper.startswith(\"PROPAGANDA\"):\n",
        "                tag = \"propaganda\"\n",
        "            elif title_upper.startswith(\"DEZINFORMARE\"):\n",
        "                tag = \"misinformation\"\n",
        "            else:\n",
        "                tag = \"unknown\"\n",
        "\n",
        "        # preluam doar sectiunea ȘTIRE\n",
        "        paragraphs = soup.select(\"div.page-content p\")\n",
        "        content = []\n",
        "        capture = False\n",
        "        for p in paragraphs:\n",
        "            strong = p.find(\"strong\")\n",
        "            strong_text = strong.get_text(strip=True).upper() if strong else \"\"\n",
        "\n",
        "            if \"ȘTIRE\" in strong_text and not capture:\n",
        "                capture = True\n",
        "                strong.extract()\n",
        "                text = p.get_text(strip=True)\n",
        "                if text:\n",
        "                    content.append(text)\n",
        "                continue\n",
        "\n",
        "            if capture:\n",
        "                if strong and re.search(r\"NARAȚIUNE|NARAȚIUNI|NARAŢIUNE|NARAŢIUNI\", strong_text, re.IGNORECASE):\n",
        "                    break\n",
        "                text = p.get_text(strip=True)\n",
        "                if text:\n",
        "                    content.append(text)\n",
        "\n",
        "        full_content = \"\\n\".join(content).strip() if content else None\n",
        "\n",
        "        if full_content:\n",
        "            return {\n",
        "                \"url\": article_url,\n",
        "                \"title\": title,\n",
        "                \"tag\": tag,\n",
        "                \"content\": full_content\n",
        "            }\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping article: {article_url}\\n{e}\")\n",
        "        return None\n",
        "\n",
        "def extract_article_links_from_page(page_num):\n",
        "    url = f\"{BASE_URL}/baza-de-date?page={page_num}\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        table = soup.find(\"table\", class_=\"rwd-table\")\n",
        "        links = []\n",
        "        if table:\n",
        "            rows = table.find_all(\"tr\")\n",
        "            for row in rows:\n",
        "                link_tag = row.find(\"a\", href=True)\n",
        "                if link_tag:\n",
        "                    href = link_tag[\"href\"]\n",
        "                    full_url = href if href.startswith(\"http\") else BASE_URL + href\n",
        "                    links.append(full_url)\n",
        "        return links\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get links from page {page_num}: {e}\")\n",
        "        return []\n",
        "\n",
        "# scrape cu thread-uri\n",
        "for page in range(START_PAGE, END_PAGE + 1):\n",
        "    print(f\"Processing page {page}...\")\n",
        "    article_urls = extract_article_links_from_page(page)\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        future_to_url = {executor.submit(scrape_article_page, url): url for url in article_urls}\n",
        "        for future in as_completed(future_to_url):\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                articles_data.append(result)\n",
        "\n",
        "# salvam in csv\n",
        "with open(\"veridica_articles.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"url\", \"title\", \"tag\", \"content\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(articles_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificam similaritatile dintre setul de pe veridica si cel online"
      ],
      "metadata": {
        "id": "1gtfib07M-2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "hf_dataset_dict = load_dataset(\"mateiaass/FakeRom\")\n",
        "hf_dataset = hf_dataset_dict[\"train\"]\n",
        "hf_df = hf_dataset.to_pandas()\n",
        "\n",
        "# Load csv local\n",
        "local_df = pd.read_csv(\"veridica_articles.csv\")\n",
        "hf_column = 'Text'\n",
        "local_column = 'content'\n",
        "\n",
        "hf_texts = hf_df[hf_column].dropna().astype(str).tolist()\n",
        "local_texts = local_df[local_column].dropna().astype(str).tolist()\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Encode texts\n",
        "hf_embeddings = model.encode(hf_texts, convert_to_tensor=True, show_progress_bar=True)\n",
        "local_embeddings = model.encode(local_texts, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_scores = util.cos_sim(local_embeddings, hf_embeddings)\n",
        "\n",
        "#definim treshhold de similaritate\n",
        "threshold = 0.8\n",
        "matches = []\n",
        "\n",
        "#cautam si pastram intrarile similare\n",
        "for i, local_score in enumerate(cosine_scores):\n",
        "    for j, score in enumerate(local_score):\n",
        "        if score >= threshold:\n",
        "            matches.append((i, j, float(score)))\n",
        "\n",
        "#printam rez\n",
        "print(f\"Found {len(matches)} similar entries with similarity >= {threshold}.\")\n",
        "print(\"Sample matches:\")\n",
        "for i, j, score in matches[:10]:  # afisam primele 10 matchuri\n",
        "    print(f\"\\n[Local {i}] {local_texts[i][:200]}...\\n[HF {j}] {hf_texts[j][:200]}...\\n→ Similarity: {score:.4f}\")\n"
      ],
      "metadata": {
        "id": "HdC65DEiM75Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preluam stirile de pe TNR"
      ],
      "metadata": {
        "id": "ct9l28R5NEj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preluam stirile satirice de pe TNR\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "\n",
        "BASE_URLS = {\n",
        "    \"politic\": \"https://www.timesnewroman.ro/politic\",\n",
        "    \"sport\": \"https://www.timesnewroman.ro/sport\",\n",
        "    \"monden\": \"https://www.timesnewroman.ro/monden\",\n",
        "    \"life-death\": \"https://www.timesnewroman.ro/life-death\",\n",
        "    \"it-stiinta\": \"https://www.timesnewroman.ro/it-stiinta\"\n",
        "}\n",
        "\n",
        "MAX_ARTICLES_PER_CATEGORY = 180\n",
        "MAX_PAGES_PER_CATEGORY = 100\n",
        "MAX_WORKERS = 10\n",
        "\n",
        "def extract_articles_from_page(category, category_url, page_num):\n",
        "    page_url = f\"{category_url}/page/{page_num}\"\n",
        "    try:\n",
        "        response = requests.get(page_url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        links = soup.select(\"a.article-url\")\n",
        "\n",
        "        articles = []\n",
        "        for a in links:\n",
        "            href = a.get(\"href\")\n",
        "            if href and href.startswith(\"https://\"):\n",
        "                articles.append({\n",
        "                    \"category\": category,\n",
        "                    \"url\": href.strip()\n",
        "                })\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch page: {page_url} -> {e}\")\n",
        "        return []\n",
        "\n",
        "def scrape_article(article_info):\n",
        "    try:\n",
        "        url = article_info[\"url\"]\n",
        "        category = article_info[\"category\"]\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # skip la articolele premium\n",
        "        if soup.find(\"div\", class_=\"join-premium-container\"):\n",
        "            return None\n",
        "\n",
        "        # Extragem titlul\n",
        "        title_tag = soup.find(\"h1\", class_=\"mb-4\")\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "\n",
        "        # si content\n",
        "        content_div = soup.find(\"div\", class_=\"content-container page-editor-content mb-3\")\n",
        "        paragraphs = content_div.find_all(\"p\") if content_div else []\n",
        "        content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
        "\n",
        "        if not content:\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"title\": title,\n",
        "            \"tag\": \"satire\",\n",
        "            \"category\": category,\n",
        "            \"content\": content\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape article: {article_info['url']} -> {e}\")\n",
        "        return None\n",
        "\n",
        "def collect_articles_for_category(category, url):\n",
        "    collected = []\n",
        "    seen_urls = set()\n",
        "    page = 1\n",
        "\n",
        "    print(f\" Collecting articles for category: {category}\")\n",
        "\n",
        "    while len(collected) < MAX_ARTICLES_PER_CATEGORY and page <= MAX_PAGES_PER_CATEGORY:\n",
        "        articles = extract_articles_from_page(category, url, page)\n",
        "        page += 1\n",
        "\n",
        "        # stergem duplicatele\n",
        "        new_articles = [a for a in articles if a['url'] not in seen_urls]\n",
        "        seen_urls.update(a['url'] for a in new_articles)\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "            results = executor.map(scrape_article, new_articles)\n",
        "\n",
        "            for result in results:\n",
        "                if result:\n",
        "                    collected.append(result)\n",
        "                    if len(collected) >= MAX_ARTICLES_PER_CATEGORY:\n",
        "                        break\n",
        "\n",
        "        print(f\"Collected {len(collected)} articles for {category}...\")\n",
        "\n",
        "        if not articles:\n",
        "            print(f\"No more articles found in {category}, stopping early.\")\n",
        "            break\n",
        "\n",
        "    return collected\n",
        "\n",
        "# preluam articolele\n",
        "all_articles = []\n",
        "for category, url in BASE_URLS.items():\n",
        "    articles = collect_articles_for_category(category, url)\n",
        "    all_articles.extend(articles)\n",
        "\n",
        "# salvam in csv\n",
        "with open(\"timesnewroman_200_per_category.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"url\", \"title\", \"tag\", \"category\", \"content\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(all_articles)\n",
        "\n",
        "print(f\"\\n Done! Total articles saved: {len(all_articles)}\")\n"
      ],
      "metadata": {
        "id": "4YT5VJsMLLsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preluam stirile de pe Digi24"
      ],
      "metadata": {
        "id": "SQxjg1OvNHTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preluam stirile reale de pe Digi24\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import csv\n",
        "\n",
        "BASE_SECTIONS = {\n",
        "    \"politica\": \"https://www.digi24.ro/stiri/actualitate/politica\",\n",
        "    \"actualitate\": \"https://www.digi24.ro/stiri/actualitate\",\n",
        "    \"economie\": \"https://www.digi24.ro/stiri/economie\",\n",
        "    \"externe\": \"https://www.digi24.ro/stiri/externe\",\n",
        "    \"sport\": \"https://www.digi24.ro/stiri/sport\",\n",
        "    \"stil-de-viata\": \"https://www.digi24.ro/magazin/stil-de-viata\"\n",
        "}\n",
        "\n",
        "PAGES_PER_SECTION = 3\n",
        "MAX_WORKERS = 10\n",
        "BASE_URL = \"https://www.digi24.ro\"\n",
        "\n",
        "def extract_article_links(section_name, section_url, page):\n",
        "    url = f\"{section_url}?p={page}\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        articles = []\n",
        "        for tag in soup.select(\"h2.article-title a\"):\n",
        "            href = tag.get(\"href\")\n",
        "            if href and href.startswith(\"/\"):\n",
        "                full_url = BASE_URL + href\n",
        "                articles.append({\n",
        "                    \"category\": section_name,\n",
        "                    \"url\": full_url\n",
        "                })\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def scrape_article(article_info):\n",
        "    try:\n",
        "        url = article_info[\"url\"]\n",
        "        category = article_info[\"category\"]\n",
        "\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # extragem titlul\n",
        "        title_tag = soup.find(\"h1\")\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "\n",
        "        # extragem paragrafele cu atributul data-index\n",
        "        paragraphs = soup.find_all(\"p\", attrs={\"data-index\": True})\n",
        "        content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
        "\n",
        "        if not content:\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"title\": title,\n",
        "            \"tag\": 'real_news',\n",
        "            \"content\": content\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape article: {article_info['url']} -> {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# luam toate linkurile de la articole\n",
        "article_links = []\n",
        "for category, base_url in BASE_SECTIONS.items():\n",
        "    for page in range(1, PAGES_PER_SECTION + 1):\n",
        "        links = extract_article_links(category, base_url, page)\n",
        "        article_links.extend(links)\n",
        "\n",
        "print(f\"Collected {len(article_links)} article URLs. Now scraping content...\")\n",
        "\n",
        "#facem scraping in paralel\n",
        "articles_data = []\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    futures = [executor.submit(scrape_article, info) for info in article_links]\n",
        "    for future in as_completed(futures):\n",
        "        result = future.result()\n",
        "        if result:\n",
        "            articles_data.append(result)\n",
        "\n",
        "# salvam in csv\n",
        "with open(\"digi24_articles.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"url\", \"title\", \"tag\", \"content\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(articles_data)\n",
        "\n",
        "print(f\"Finished: {len(articles_data)} articles saved to digi24_articles.csv\")\n"
      ],
      "metadata": {
        "id": "OVU_THPdLW1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMBINAM TOATE DATELE INTR UN SINGUR SET"
      ],
      "metadata": {
        "id": "S4qM3Ug2NnCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "hf_dataset_dict = load_dataset(\"mateiaass/FakeRom\")\n",
        "hf_dataset = hf_dataset_dict[\"train\"]\n",
        "df = hf_dataset.to_pandas()\n",
        "\n",
        "# Keep only 'Folder' and 'Text' columns\n",
        "df = df[[\"Folder\", \"Text\"]].copy()\n",
        "\n",
        "# Remove the exact string \"---Vaccin Sample---\" (but keep surrounding text)\n",
        "df[\"Text\"] = df[\"Text\"].str.replace(\"---Vaccin Sample---\", \"\", regex=False).str.strip()\n",
        "\n",
        "# Map 'Folder' to custom tags\n",
        "folder_to_tag = {\n",
        "    \"stiri_reale_img\": \"real_news\",\n",
        "    \"stiri_satirice_img\": \"satire\",\n",
        "    \"stiri_propagandistice_img\": \"propaganda\",\n",
        "    \"stiri_fabricate_img\": \"fake_news\",\n",
        "    \"stiri_plauzibile_img\": \"misinformation\"\n",
        "}\n",
        "df[\"tag\"] = df[\"Folder\"].map(folder_to_tag)\n",
        "\n",
        "# Final cleanup and renaming\n",
        "df = df.rename(columns={\"Text\": \"content\"})\n",
        "df = df[[\"tag\", \"content\"]]\n",
        "df = df.dropna(subset=[\"tag\", \"content\"])\n",
        "\n",
        "# Load additional local datasets\n",
        "veridica_df = pd.read_csv(\"veridica_articles.csv\")\n",
        "digi24_df = pd.read_csv(\"digi24_articles_deduplicated.csv\")\n",
        "times_df = pd.read_csv(\"timesnewroman_200_per_category.csv\")\n",
        "\n",
        "# Check column existence\n",
        "for name, dataset in [(\"veridica\", veridica_df), (\"digi24\", digi24_df), (\"timesnewroman\", times_df)]:\n",
        "    if not {\"tag\", \"content\"}.issubset(dataset.columns):\n",
        "        raise ValueError(f\"The {name} dataset must contain 'tag' and 'content' columns.\")\n",
        "\n",
        "# Combine all datasets\n",
        "combined_df = pd.concat([\n",
        "    df,\n",
        "    veridica_df[[\"tag\", \"content\"]],\n",
        "    digi24_df[[\"tag\", \"content\"]],\n",
        "    times_df[[\"tag\", \"content\"]]\n",
        "], ignore_index=True)\n",
        "\n",
        "# Shuffle the dataset\n",
        "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the final dataset\n",
        "combined_df.to_csv(\"augmented_fake_rom_combined.csv\", index=False)\n",
        "\n",
        "# Summary\n",
        "print(\"Final combined dataset saved as 'augmented_fake_rom_combined.csv'.\")\n",
        "print(f\"Total samples: {len(combined_df)}\")\n",
        "print(\"Tag distribution:\")\n",
        "print(combined_df['tag'].value_counts())\n"
      ],
      "metadata": {
        "id": "rJsAtpo5NmV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splittuirea textelor pt a le aduce la acelasi numar"
      ],
      "metadata": {
        "id": "2H9bsNtWMoOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load spaCy model for Romanian\n",
        "nlp = spacy.load(\"ro_core_news_sm\")\n",
        "\n",
        "# Load and preprocess dataset\n",
        "df = pd.read_csv(\"augmented_fake_rom_combined.csv\")\n",
        "\n",
        "# Remove exact duplicates based on \"content\"\n",
        "df = df.drop_duplicates(subset=\"content\").copy()\n",
        "\n",
        "# Compute content length and sort descending\n",
        "df[\"content_length\"] = df[\"content\"].str.len()\n",
        "df = df.sort_values(by=\"content_length\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Parameters\n",
        "tags_to_split = {\"propaganda\", \"fake_news\", \"misinformation\"}\n",
        "max_length = 512\n",
        "chunking_limit = 1000\n",
        "\n",
        "# Initialize tag counters\n",
        "initial_tag_counts = df[\"tag\"].value_counts().to_dict()\n",
        "current_tag_counts = initial_tag_counts.copy()\n",
        "\n",
        "# To track how many extra chunks we’ve added per tag\n",
        "new_tag_counts = defaultdict(int)\n",
        "\n",
        "# Store final processed rows\n",
        "final_rows = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    tag = row[\"tag\"]\n",
        "    content = str(row[\"content\"]).strip()\n",
        "\n",
        "    # Skip processing if tag is not eligible for chunking\n",
        "    if tag not in tags_to_split:\n",
        "        final_rows.append({\"tag\": tag, \"content\": content})\n",
        "        continue\n",
        "\n",
        "    # Check if chunking is still allowed for this tag\n",
        "    if current_tag_counts[tag] >= chunking_limit or len(content) <= max_length:\n",
        "        final_rows.append({\"tag\": tag, \"content\": content})\n",
        "        continue\n",
        "\n",
        "    # Split into sentences and chunk\n",
        "    doc = nlp(content)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sent in sentences:\n",
        "        if len(current_chunk) + len(sent) + 1 <= max_length:\n",
        "            current_chunk += (\" \" if current_chunk else \"\") + sent\n",
        "        else:\n",
        "            chunks.append(current_chunk)\n",
        "            current_chunk = sent\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "    # If splitting this text will exceed the limit, skip and keep original\n",
        "    if current_tag_counts[tag] - 1 + len(chunks) > chunking_limit:\n",
        "        final_rows.append({\"tag\": tag, \"content\": content})\n",
        "        continue\n",
        "\n",
        "    # Otherwise, accept the chunked version\n",
        "    for chunk in chunks:\n",
        "        final_rows.append({\"tag\": tag, \"content\": chunk})\n",
        "    current_tag_counts[tag] = current_tag_counts[tag] - 1 + len(chunks)\n",
        "    new_tag_counts[tag] += len(chunks) - 1  # for summary\n",
        "\n",
        "# Convert to DataFrame and shuffle\n",
        "new_df = pd.DataFrame(final_rows)\n",
        "new_df = new_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save result\n",
        "new_df.to_csv(\"augmented_fake_rom_combined_correctly_split.csv\", index=False)\n",
        "\n",
        "# Print summary\n",
        "print(\"Processed and saved to 'augmented_fake_rom_combined_correctly_split.csv'.\")\n",
        "print(f\"Original rows: {len(df)} → Final rows (with augmentation): {len(new_df)}\")\n",
        "print(\"Initial tag distribution:\")\n",
        "print(pd.Series(initial_tag_counts))\n",
        "print(\"Final tag distribution:\")\n",
        "print(new_df['tag'].value_counts())\n",
        "print(\"Augmentation impact (added chunks minus originals replaced):\")\n",
        "print(pd.Series(new_tag_counts))"
      ],
      "metadata": {
        "id": "2Zr5isjiMwS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cream setul de date tradus\n",
        "1. Facem traducerile\n",
        "2. Veridicam similaritatea"
      ],
      "metadata": {
        "id": "T7Ejg1auOF6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traducem setul de date ro->zh->ro"
      ],
      "metadata": {
        "id": "MS-NgMfWOSlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup si import-uri\n",
        "!pip install google-cloud-translate==2.0.1 --quiet\n",
        "!pip install pandas tqdm --quiet\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from google.api_core import exceptions\n",
        "from google.cloud import translate_v2 as translate\n",
        "\n",
        "# 2. Configuratie\n",
        "GCP_PROJECT_ID = \"translate\"\n",
        "PATH_TO_GCP_KEY = \"/content/strange-bird-462911-i4-468a1f01f318.json\"\n",
        "PATH_TO_DATASET = \"augmented_fake_rom_combined_correctly_split.csv\"\n",
        "OUTPUT_CSV_PATH = \"5_class_back_translated_dataset.csv\"\n",
        "ERROR_LOG_PATH = \"translation_errors.log\"\n",
        "\n",
        "SOURCE_LANGUAGE = 'ro'\n",
        "INTERMEDIATE_LANGUAGE = 'zh-CN'\n",
        "FALLBACK_LANGUAGE = 'en'\n",
        "TARGET_LANGUAGE = 'ro'\n",
        "DELAY_BETWEEN_CALLS = 0.1\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Verificam configuratia\n",
        "if GCP_PROJECT_ID == \"your-gcp-project-id\" or PATH_TO_GCP_KEY == \"your-gcp-key-file.json\":\n",
        "    print(\"WARNING: Update GCP_PROJECT_ID si PATH_TO_GCP_KEY.\")\n",
        "if not os.path.exists(PATH_TO_DATASET):\n",
        "    print(f\"ERROR: Datasetul nu a fost gasit la '{PATH_TO_DATASET}'.\")\n",
        "if not os.path.exists(PATH_TO_GCP_KEY):\n",
        "    print(f\"ERROR: Cheia GCP nu a fost gasita la '{PATH_TO_GCP_KEY}'.\")\n",
        "\n",
        "# 3. Autentificare Google Cloud\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = PATH_TO_GCP_KEY\n",
        "translate_client = translate.Client()\n",
        "\n",
        "# 4. Functie de traducere cu retry\n",
        "\n",
        "def safe_translate(text, source, target, retries=MAX_RETRIES):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            result = translate_client.translate(text, source_language=source, target_language=target)\n",
        "            return result['translatedText'], None\n",
        "        except exceptions.GoogleAPICallError as e:\n",
        "            if attempt < retries - 1:\n",
        "                sleep_time = DELAY_BETWEEN_CALLS * (2 ** attempt + random.random())\n",
        "                time.sleep(sleep_time)\n",
        "            else:\n",
        "                return text, f\"API_ERROR - {e.message}\"\n",
        "        except Exception as e:\n",
        "            return text, f\"GENERAL_ERROR - {str(e)}\"\n",
        "\n",
        "\n",
        "def back_translate(text: str, src_lang: str, intermediate_lang: str, target_lang: str):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text) if not pd.isna(text) else \"\"\n",
        "\n",
        "    if not text.strip():\n",
        "        return text, \"SKIP - input invalid sau gol\"\n",
        "\n",
        "    # Prima incercare cu limba intermediara principala\n",
        "    intermediate, error = safe_translate(text, src_lang, intermediate_lang)\n",
        "    if error:\n",
        "        # Incearca fallback (engleza) daca chineza esueaza\n",
        "        intermediate, fallback_error = safe_translate(text, src_lang, FALLBACK_LANGUAGE)\n",
        "        if fallback_error:\n",
        "            return text, f\"FAILED_BOTH - {error} | FALLBACK - {fallback_error}\"\n",
        "        intermediate_lang = FALLBACK_LANGUAGE\n",
        "\n",
        "    final, error = safe_translate(intermediate, intermediate_lang, target_lang)\n",
        "    return final if not error else text, error\n",
        "\n",
        "# 5. Procesare dataset\n",
        "back_translated_content = []\n",
        "error_log = []\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(PATH_TO_DATASET)\n",
        "    print(f\"Incarcat datasetul: {df.shape} linii.\")\n",
        "\n",
        "    # Asiguram ca 'content' este text\n",
        "    df['content'] = df['content'].astype(str).fillna(\"\")\n",
        "\n",
        "    for row in tqdm(df.itertuples(index=True), total=len(df), desc=\"Translating entries\"):\n",
        "        index = row.Index\n",
        "        original_text = row.content\n",
        "\n",
        "        if not isinstance(original_text, str):\n",
        "            print(f\"Warning: entry {index} is not a string (type={type(original_text)}). Converting.\")\n",
        "\n",
        "        translated_text, error = back_translate(\n",
        "            original_text,\n",
        "            SOURCE_LANGUAGE,\n",
        "            INTERMEDIATE_LANGUAGE,\n",
        "            TARGET_LANGUAGE\n",
        "        )\n",
        "\n",
        "        back_translated_content.append(translated_text)\n",
        "\n",
        "        if error:\n",
        "            error_log.append(f\"{index},{error},{original_text[:100].replace(',', ' ')}\")\n",
        "\n",
        "        time.sleep(DELAY_BETWEEN_CALLS)\n",
        "\n",
        "    df['back_translated_content'] = back_translated_content\n",
        "    print(\"\\nTraducerea inversa s-a terminat cu succes.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Fisierul '{PATH_TO_DATASET}' nu a fost gasit.\")\n",
        "except KeyError:\n",
        "    print(\"ERROR: CSV-ul trebuie sa contina o coloana numita 'content'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error: {e}\")\n",
        "\n",
        "# 6. Salvare rezultate\n",
        "if 'df' in locals() and not df.empty:\n",
        "    df.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8')\n",
        "    print(f\"Salvat datasetul augmentat la '{OUTPUT_CSV_PATH}'\")\n",
        "\n",
        "    if error_log:\n",
        "        with open(ERROR_LOG_PATH, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"index,error,message\\n\")\n",
        "            f.write(\"\\n\".join(error_log))\n",
        "        print(f\"Erorile au fost salvate in '{ERROR_LOG_PATH}'\")\n",
        "    else:\n",
        "        print(\"Nicio eroare nu a fost intalnita in timpul traducerii.\")"
      ],
      "metadata": {
        "id": "fN5IYbkROFFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizam similaritatea semantica"
      ],
      "metadata": {
        "id": "VOOVXhqGObDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. setup si import-uri necesare\n",
        "#!pip install pandas --quiet\n",
        "#!pip install sentence-transformers --quiet\n",
        "#!pip install matplotlib seaborn --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Libraries installed and imported successfully.\")\n",
        "\n",
        "# 2. configuratia\n",
        "PATH_TO_BACK_TRANSLATED_DATASET = \"5_class_back_translated_dataset.csv\"\n",
        "FINAL_CSV_WITH_SIMILARITY_PATH = \"similarity_report.csv\"\n",
        "\n",
        "# folosim un model multilingvistic care s mearga la limba romana\n",
        "SIMILARITY_MODEL_NAME = 'distiluse-base-multilingual-cased-v1'\n",
        "\n",
        "if not os.path.exists(PATH_TO_BACK_TRANSLATED_DATASET):\n",
        "    print(f\"ERROR: Dataset file not found at '{PATH_TO_BACK_TRANSLATED_DATASET}'.\")\n",
        "    print(\"Please make sure the file path is correct.\")\n",
        "\n",
        "# 3. Incarcam modelul si datele\n",
        "try:\n",
        "    # incarcam setul tradus\n",
        "    df_results = pd.read_csv(PATH_TO_BACK_TRANSLATED_DATASET)\n",
        "    print(f\"Dataset '{PATH_TO_BACK_TRANSLATED_DATASET}' loaded successfully. Shape: {df_results.shape}\")\n",
        "\n",
        "    # eliminam linia 2448 daca exista\n",
        "    if 2124 in df_results.index:\n",
        "        df_results = df_results.drop(index=2124)\n",
        "        print(\"Row 2448 has been dropped.\")\n",
        "\n",
        "    # verificam existenta coloanelor\n",
        "    required_cols = ['tag','content', 'back_translated_content']\n",
        "    if not all(col in df_results.columns for col in required_cols):\n",
        "        raise KeyError(\"The CSV must contain 'content', 'back_translated_content', and 'tag' columns.\")\n",
        "\n",
        "    # incarcam modelul sentence transformer\n",
        "    print(f\"\\nLoading sentence transformer model '{SIMILARITY_MODEL_NAME}'...\")\n",
        "    print(\"This may take a few moments.\")\n",
        "    model = SentenceTransformer(SIMILARITY_MODEL_NAME)\n",
        "    print(\" Model loaded successfully.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\" FATAL ERROR: The file '{PATH_TO_BACK_TRANSLATED_DATASET}' was not found.\")\n",
        "except KeyError as e:\n",
        "    print(f\" FATAL ERROR: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "# 4. calculam similaritatea semantica\n",
        "if 'model' in locals() and 'df_results' in locals():\n",
        "    print(\"\\n--- Starting Semantic Similarity Calculation ---\")\n",
        "    try:\n",
        "        # ne asiguram ca coloanele sunt tratate ca string-uri pentru a nu avea erori\n",
        "        original_texts = df_results['content'].fillna('').astype(str).tolist()\n",
        "        back_translated_texts = df_results['back_translated_content'].fillna('').astype(str).tolist()\n",
        "\n",
        "        # generam embedding-uri pentru ambele texte\n",
        "        print(\"Generating embeddings for original texts...\")\n",
        "        embeddings1 = model.encode(original_texts, convert_to_tensor=True, show_progress_bar=True)\n",
        "        print(\"\\nGenerating embeddings for back-translated texts...\")\n",
        "        embeddings2 = model.encode(back_translated_texts, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "        # calculam similaritatea cosinus\n",
        "        print(\"\\nCalculating cosine similarity scores...\")\n",
        "        cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "        #(diagonala matricei de similaritate)\n",
        "        similarities = [cosine_scores[i][i].item() for i in range(len(original_texts))]\n",
        "\n",
        "        # adaugam similaritatile ca o coloana noua in dataset\n",
        "        df_results['semantic_similarity'] = similarities\n",
        "        print(\"Similarity calculation complete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" An error occurred during similarity calculation: {e}\")\n",
        "else:\n",
        "    print(\"\\nSkipping calculation because the model or data was not loaded correctly.\")\n",
        "\n",
        "\n",
        "# 5. analizam si salvam rezultatele\n",
        "if 'semantic_similarity' in df_results.columns:\n",
        "    print(\"\\n--- Overall Analysis of Semantic Similarity Scores ---\")\n",
        "    average_similarity = df_results['semantic_similarity'].mean()\n",
        "    median_similarity = df_results['semantic_similarity'].median()\n",
        "    std_dev = df_results['semantic_similarity'].std()\n",
        "    min_similarity = df_results['semantic_similarity'].min()\n",
        "    max_similarity = df_results['semantic_similarity'].max()\n",
        "\n",
        "    print(f\"Average Similarity: {average_similarity:.4f}\")\n",
        "    print(f\"Median Similarity:  {median_similarity:.4f}\")\n",
        "    print(f\"Standard Deviation: {std_dev:.4f}\")\n",
        "    print(f\"Minimum Similarity: {min_similarity:.4f}\")\n",
        "    print(f\"Maximum Similarity: {max_similarity:.4f}\")\n",
        "\n",
        "    # analiza de grup dupa tag\n",
        "    print(\"\\n--- Average Similarity per Tag Class ---\")\n",
        "    try:\n",
        "        # grupam dupa tag si calculam media pt semantic_similarity\n",
        "        similarity_by_tag = df_results.groupby('tag')['semantic_similarity'].mean().sort_values(ascending=False)\n",
        "        print(similarity_by_tag.to_string())\n",
        "    except KeyError:\n",
        "        print(\"Could not perform grouped analysis because 'tag' column was not found.\")\n",
        "\n",
        "\n",
        "        # grafic de distributie\n",
        "    # Custom color mapping for each tag\n",
        "    custom_palette = {\n",
        "        'fake_news': 'tab:blue',\n",
        "        'misinformation': 'tab:orange',\n",
        "        'propaganda': 'tab:green',\n",
        "        'real_news': 'tab:red',\n",
        "        'satire': 'tab:purple'\n",
        "    }\n",
        "\n",
        "    # Plot: clearer and less cluttered\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    sns.histplot(\n",
        "        data=df_results,\n",
        "        x='semantic_similarity',\n",
        "        hue='tag',\n",
        "        bins=25,\n",
        "        multiple='dodge',  # separate bars\n",
        "        palette=custom_palette,\n",
        "        alpha=0.6,\n",
        "        edgecolor='black'\n",
        "    )\n",
        "\n",
        "\n",
        "    # salvam setul final\n",
        "    df_results.to_csv(FINAL_CSV_WITH_SIMILARITY_PATH, index=False, encoding='utf-8')\n",
        "    print(f\"\\n Final dataset with similarity scores saved to '{FINAL_CSV_WITH_SIMILARITY_PATH}'\")\n",
        "\n",
        "    # afisam top 5 cele mai putin similare\n",
        "    print(\"\\n--- Top 5 lowest similarity examples for manual review ---\")\n",
        "    print(df_results.nsmallest(5, 'semantic_similarity')[['tag', 'content', 'back_translated_content', 'semantic_similarity']])\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping analysis because similarity scores were not calculated.\")\n"
      ],
      "metadata": {
        "id": "pI2f5DGrOWcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Incercarile nereusite de> introdus zgomot si redus numarul de clase la 3\n"
      ],
      "metadata": {
        "id": "ZeZffQ8eLmmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#incercarea cu setul cu 3 clase care n a mers\n",
        "\n",
        "import csv\n",
        "\n",
        "# Input files\n",
        "input_files = [\n",
        "    \"digi24_articles.csv\",\n",
        "    \"timesnewroman_200_per_category.csv\",\n",
        "    \"veridica_articles.csv\"\n",
        "]\n",
        "\n",
        "# Output file\n",
        "output_file = \"3_class_dataset.csv\"\n",
        "\n",
        "# Columns to keep\n",
        "fieldnames = [\"tag\", \"content\"]\n",
        "\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for file in input_files:\n",
        "        with open(file, newline='', encoding='utf-8') as infile:\n",
        "            reader = csv.DictReader(infile)\n",
        "            for row in reader:\n",
        "                content = row.get(\"content\", \"\").strip()\n",
        "                if not content:\n",
        "                    continue\n",
        "\n",
        "                if file == \"veridica_articles.csv\":\n",
        "                    tag = \"fake_news\"\n",
        "                else:\n",
        "                    tag = row.get(\"tag\", \"\").strip()\n",
        "\n",
        "                if tag:\n",
        "                    writer.writerow({\"tag\": tag, \"content\": content})\n",
        "\n",
        "print(f\"Combined dataset saved to: {output_file}\")\n"
      ],
      "metadata": {
        "id": "L0LpcmkqLx7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experimentul de introdus zgomot"
      ],
      "metadata": {
        "id": "Z1iDI2XoLuxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "DATA_PATH = \"3_class_dataset.csv\"\n",
        "RANDOM_SEED = 42\n",
        "AUGMENT_FRACTION = 1.0  # 1.0 = augmentarea intregului set de antrenare\n",
        "\n",
        "# seed pt reproductibilitate\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# load si stergem duplicatele\n",
        "print(\"Loading and deduplicating data...\")\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(f\"Original dataset size: {len(df)}\")\n",
        "\n",
        "df = df.drop_duplicates(subset=\"content\").reset_index(drop=True)\n",
        "print(f\"After deduplication: {len(df)}\")\n",
        "\n",
        "# encodam labelurile\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"tag\"])\n",
        "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"Label mapping:\", label_mapping)\n",
        "print(\"Class distribution:\")\n",
        "print(df[\"tag\"].value_counts())\n",
        "\n",
        "#stratificam setul de date\n",
        "# 80% train, 10% val, 10% test\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.2, stratify=df[\"label\"], random_state=RANDOM_SEED\n",
        ")\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset splits:\")\n",
        "print(f\"Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Val: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Test: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "\n",
        "# definim functiile de noise pt augmentare\n",
        "def word_dropout(text, p=0.1):\n",
        "    \"\"\"scoatem cuvinte random din text\"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) <= 1:  # pt a nu modifica textele prea scurte\n",
        "        return text\n",
        "\n",
        "    kept_words = [w for w in words if random.random() > p]\n",
        "    # ne asiguram ca macar un cuvant ramane\n",
        "    if not kept_words:\n",
        "        kept_words = [random.choice(words)]\n",
        "    return \" \".join(kept_words)\n",
        "\n",
        "def random_swap(text, n=1):\n",
        "    \"\"\"facem swap intre cuvinte random\"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) < 2:\n",
        "        return text\n",
        "\n",
        "    words_copy = words.copy()\n",
        "    for _ in range(min(n, len(words_copy) - 1)):\n",
        "        idx = random.randint(0, len(words_copy) - 2)\n",
        "        words_copy[idx], words_copy[idx + 1] = words_copy[idx + 1], words_copy[idx]\n",
        "    return \" \".join(words_copy)\n",
        "\n",
        "def noisy_augment(text):\n",
        "    \"\"\"aplicam noise dupa o probabilitate\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    augmented_text = text\n",
        "    if random.random() < 0.3:\n",
        "        augmented_text = word_dropout(augmented_text, p=0.1)\n",
        "    if random.random() < 0.3:\n",
        "        augmented_text = random_swap(augmented_text, n=1)\n",
        "    return augmented_text\n",
        "\n",
        "# Augmentam datele de train\n",
        "print(\"\\nApplying data augmentation...\")\n",
        "augmented_train_df = train_df.copy()\n",
        "augmented_train_df[\"content\"] = augmented_train_df[\"content\"].apply(noisy_augment)\n",
        "\n",
        "# stergem orice duplicate facute de augmentare\n",
        "augmented_train_df = augmented_train_df[\n",
        "    ~augmented_train_df[\"content\"].isin(train_df[\"content\"])\n",
        "].reset_index(drop=True)\n",
        "\n",
        "print(f\"Unique augmented samples: {len(augmented_train_df)}\")\n",
        "\n",
        "if AUGMENT_FRACTION > 0:\n",
        "    samples_to_add = int(len(augmented_train_df) * AUGMENT_FRACTION)\n",
        "    augmented_subset = augmented_train_df.sample(\n",
        "        n=min(samples_to_add, len(augmented_train_df)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    final_train_df = pd.concat([train_df, augmented_subset], ignore_index=True)\n",
        "else:\n",
        "    final_train_df = train_df.copy()\n",
        "\n",
        "# shuffle la ttrain\n",
        "final_train_df = final_train_df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "\n",
        "# -Validam si salvam setul de date\n",
        "print(\"\\nFinal dataset sizes:\")\n",
        "print(f\"  Train (with augmentation): {len(final_train_df)} samples\")\n",
        "print(f\"  Validation: {len(val_df)} samples\")\n",
        "print(f\"  Test: {len(test_df)} samples\")\n",
        "\n",
        "# verificam pt leakage\n",
        "train_content = set(final_train_df[\"content\"])\n",
        "val_content = set(val_df[\"content\"])\n",
        "test_content = set(test_df[\"content\"])\n",
        "\n",
        "train_val_overlap = len(train_content & val_content)\n",
        "train_test_overlap = len(train_content & test_content)\n",
        "val_test_overlap = len(val_content & test_content)\n",
        "\n",
        "if train_val_overlap > 0 or train_test_overlap > 0 or val_test_overlap > 0:\n",
        "    print(f\" Data leakage detected:\")\n",
        "    print(f\"  Train-Val overlap: {train_val_overlap}\")\n",
        "    print(f\"  Train-Test overlap: {train_test_overlap}\")\n",
        "    print(f\"  Val-Test overlap: {val_test_overlap}\")\n",
        "else:\n",
        "    print(\" No data leakage detected\")\n",
        "\n",
        "#salvam seturile noi\n",
        "final_train_df.to_csv(\"train_noisy.csv\", index=False)\n",
        "val_df.to_csv(\"val.csv\", index=False)\n",
        "test_df.to_csv(\"test.csv\", index=False)\n",
        "\n",
        "import json\n",
        "label_mapping_serializable = {k: int(v) for k, v in label_mapping.items()}\n",
        "with open(\"label_mapping.json\", \"w\") as f:\n",
        "    json.dump(label_mapping_serializable, f, indent=2)\n",
        "\n",
        "print(\"\\n Data prepared and saved:\")\n",
        "print(\"  Files: train_noisy.csv, val.csv, test.csv, label_mapping.json\")"
      ],
      "metadata": {
        "id": "phbCYB6QLtg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline SVC si MNB\n",
        "Include si testarile pt setul de date tradus"
      ],
      "metadata": {
        "id": "pzxexqTFMCd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cautam hiperparametrii optimi si antrenam modelele"
      ],
      "metadata": {
        "id": "hC7RhQf5MMmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# setup stopwords\n",
        "try:\n",
        "    stopwords_set = set(nltk.corpus.stopwords.words('romanian'))\n",
        "except LookupError:\n",
        "    print(\"Downloading 'stopwords' corpus...\")\n",
        "    nltk.download('stopwords')\n",
        "    stopwords_set = set(nltk.corpus.stopwords.words('romanian'))\n",
        "\n",
        "# functii de preprocesare si curatare\n",
        "\n",
        "def clean_for_ml(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^\\w\\săâîșț-]\", \"\", text, flags=re.UNICODE)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def scrape_and_clean(url):\n",
        "    try:\n",
        "        page = requests.get(url)\n",
        "        if page.status_code != 200:\n",
        "            return {\"error\": f\"Failed to fetch page. Status Code: {page.status_code}\"}\n",
        "\n",
        "        soup = BeautifulSoup(page.text, 'html.parser')\n",
        "        paragraph = \" \".join([p.text.strip() for p in soup.find_all('p')])\n",
        "\n",
        "        # aplicam curatarea\n",
        "        cleaned_text = clean_for_ml(paragraph)\n",
        "        return cleaned_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "\n",
        "# load dataset si splittuim datele\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(\"augmented_fake_rom_combined_correctly_split.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'augmented_fake_rom_chunked_filtered_dedup.csv' not found. Please make sure the file is in the correct directory.\")\n",
        "    exit()\n",
        "\n",
        "df = df.dropna(subset=[\"content\"])\n",
        "df[\"content_cleaned\"] = df[\"content\"].astype(str).apply(clean_for_ml)\n",
        "df = df[df[\"content_cleaned\"].str.strip() != \"\"]\n",
        "\n",
        "X = df[\"content_cleaned\"]\n",
        "y_labels = df[\"tag\"]\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y_labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# tunarea hiperparametrilor cu gridsearch\n",
        "\n",
        "# definim strategia de cross fold\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "print(\"Tuning Support Vector Machine...\")\n",
        "pipeline_svm = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=list(stopwords_set))),\n",
        "    ('svm', SVC(kernel='linear', random_state=42))\n",
        "])\n",
        "\n",
        "# definim gridul de parametrii\n",
        "param_grid_svm = {\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "    'svm__C': [ 1 , 0.01, 0.1, 10]\n",
        "}\n",
        "\n",
        "\n",
        "# instantiem gridul\n",
        "search_svm = GridSearchCV(\n",
        "    pipeline_svm,\n",
        "    param_grid_svm,\n",
        "    cv=cv,\n",
        "    scoring='f1-macro',\n",
        ")\n",
        "search_svm.fit(X_train, y_train)\n",
        "best_svm = search_svm.best_estimator_\n",
        "\n",
        "print(f\"\\nBest parameters for SVM: {search_svm.best_params_}\")\n",
        "print(f\"Best cross-validated F1-score for SVM: {search_svm.best_score_:.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "print(\"Tuning Multinomial Naive Bayes...\")\n",
        "pipeline_nb = Pipeline([\n",
        "    ('countvec', CountVectorizer(stop_words=list(stopwords_set))),\n",
        "    ('nb', MultinomialNB())\n",
        "])\n",
        "\n",
        "param_grid_nb = {\n",
        "    #'countvec__ngram_range': [(1, 1)],\n",
        "    'nb__alpha': [0.01, 0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "\n",
        "search_nb = GridSearchCV(\n",
        "    pipeline_nb,\n",
        "    param_grid_nb,\n",
        "    cv=cv,\n",
        "    scoring='f1-macro',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "search_nb.fit(X_train, y_train)\n",
        "best_nb = search_nb.best_estimator_\n",
        "\n",
        "print(f\"\\nBest parameters for MNB: {search_nb.best_params_}\")\n",
        "print(f\"Best cross-validated F1-score for MNB: {search_nb.best_score_:.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# evaluarea finala pe test\n",
        "\n",
        "print(\"\\n--- Final Evaluation on the Unseen Test Set ---\")\n",
        "\n",
        "# SVM eval\n",
        "y_pred_svm = best_svm.predict(X_test)\n",
        "print(\"Test Set Classification Report for Best SVM:\\n\")\n",
        "print(classification_report(y_test, y_pred_svm, target_names=label_encoder.classes_))\n",
        "\n",
        "# MNB eval\n",
        "y_pred_nb = best_nb.predict(X_test)\n",
        "print(\"Test Set Classification Report for Best Multinomial Naive Bayes:\\n\")\n",
        "print(classification_report(y_test, y_pred_nb, target_names=label_encoder.classes_))\n",
        "\n",
        "svm_vectorizer = best_svm.named_steps['tfidf']\n",
        "nb_vectorizer = best_nb.named_steps['countvec']\n",
        "\n",
        "# salvam cel mai bun model si artefactele\n",
        "print(\"\\nSaving best models and artifacts...\")\n",
        "joblib.dump(best_svm, 'best_svm_pipeline.pkl')\n",
        "joblib.dump(best_nb, 'best_nb_pipeline.pkl')\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "joblib.dump(svm_vectorizer, 'svm_vectorizer.pkl')\n",
        "joblib.dump(nb_vectorizer, 'nb_vectorizer.pkl')\n",
        "print(\"Vectorizers saved successfully.\")\n",
        "print(\"Artifacts saved successfully.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "#functie pt consfusion matrix\n",
        "def plot_conf_matrix(y_true, y_pred, model_name, labels):\n",
        "    \"\"\"Helper function to plot a confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "    disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
        "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_conf_matrix(y_test, y_pred_svm, \"SVM original dataset\", label_encoder.classes_)\n",
        "plot_conf_matrix(y_test, y_pred_nb, \"MultinomialNB original dataset\", label_encoder.classes_)\n",
        "\n",
        "\n",
        "# predictii pt 3 exemple\n",
        "def predict_news_type(pipeline, text_input):\n",
        "\n",
        "    pred_encoded = pipeline.predict([text_input])\n",
        "    return label_encoder.inverse_transform(pred_encoded)[0]\n",
        "\n",
        "print(\"\\n--- Example Predictions on New Data ---\")\n",
        "try:\n",
        "    url1 = 'https://www.timesnewroman.ro/politic/cine-este-coco-papagalul-care-a-invatat-o-pe-anamaria-gavrila-sa-spuna-pacea-i-pace/'\n",
        "    url2 = 'https://www.digi24.ro/alegeri-prezidentiale-2025/alegeri-prezidentiale-2025-turul-2-3243031'\n",
        "    url3 = 'https://www.activenews.ro/opinii/Kievul-tocmai-a-aruncat-in-aer-Negocierile-de-Pace-–-Romania-in-pericol-maxim-195903'\n",
        "\n",
        "    text1 = scrape_and_clean(url1)\n",
        "    text2 = scrape_and_clean(url2)\n",
        "    text3 = scrape_and_clean(url3)\n",
        "\n",
        "    print(f\"Best SVM Prediction (case 1): {predict_news_type(best_svm, text1)}\")\n",
        "    print(f\"Best SVM Prediction (case 2): {predict_news_type(best_svm, text2)}\")\n",
        "    print(f\"Best SVM Prediction (case 3): {predict_news_type(best_svm, text3)}\")\n",
        "    print(\"-\" * 15)\n",
        "    print(f\"Best NB Prediction (case 1): {predict_news_type(best_nb, text1)}\")\n",
        "    print(f\"Best NB Prediction (case 2): {predict_news_type(best_nb, text2)}\")\n",
        "    print(f\"Best NB Prediction (case 3): {predict_news_type(best_nb, text3)}\")\n",
        "\n",
        "except NotImplementedError as e:\n",
        "    print(f\"\\nCould not run example predictions: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during prediction: {e}\")\n"
      ],
      "metadata": {
        "id": "RJRuj3L4MHTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testam MNB pe setul tradus"
      ],
      "metadata": {
        "id": "ZFqfND0MMSQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#functia de preprocess\n",
        "def clean_for_ml(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^\\w\\săâîșț-]\", \"\", text, flags=re.UNICODE)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# incarcam pipeline u pr mnb\n",
        "print(\"Loading saved models and encoders...\")\n",
        "model = joblib.load(\"best_nb_pipeline.pkl\")\n",
        "original_classes = ['fake_news', 'misinformation', 'propaganda', 'real_news', 'satire']\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.classes_ = np.array(original_classes)\n",
        "print(\"Loaded model and label encoder.\")\n",
        "\n",
        "# load si preparam setul\n",
        "print(\"Loading and preparing dataset...\")\n",
        "df = pd.read_csv(\"5_class_back_translated_dataset.csv\")\n",
        "df = df.dropna(subset=[\"back_translated_content\"])\n",
        "df[\"content_cleaned\"] = df[\"back_translated_content\"].astype(str).apply(clean_for_ml)\n",
        "df = df[df[\"content_cleaned\"].str.strip() != \"\"]\n",
        "\n",
        "X = df[\"content_cleaned\"]\n",
        "y_str = df[\"tag\"]\n",
        "\n",
        "# ne asiguram ca labelurile sunt bune\n",
        "y = label_encoder.transform(y_str)\n",
        "\n",
        "# stratificam setul la fel ca mai sus\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Running predictions...\")\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluare\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "def plot_conf_matrix(y_true, y_pred, model_name, labels):\n",
        "    \"\"\"Helper function to plot a confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "    plot_obj = disp.plot(cmap=\"OrRd\", xticks_rotation=45)\n",
        "    plot_obj.ax_.grid(False)\n",
        "    plot_obj.ax_.set_title(f\"Confusion Matrix - MultinomialNB back-translated set\", pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_conf_matrix(y_test, y_pred, \"MNB back-translated set\", label_encoder.classes_)"
      ],
      "metadata": {
        "id": "VkDM8-HwMRc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testam SVC pe setul tradus"
      ],
      "metadata": {
        "id": "qLbrgxP9MXsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# preprocesam cu ac functie\n",
        "def clean_for_ml(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^\\w\\săâîșț-]\", \"\", text, flags=re.UNICODE)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# incarcam pipeline u\n",
        "print(\"Loading saved models and encoders...\")\n",
        "model = joblib.load(\"best_svm_pipeline.pkl\")\n",
        "original_classes = ['fake_news', 'misinformation', 'propaganda', 'real_news', 'satire']\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.classes_ = np.array(original_classes)\n",
        "print(\"Loaded model and label encoder.\")\n",
        "\n",
        "# load si pregatim setul\n",
        "print(\"Loading and preparing dataset...\")\n",
        "df = pd.read_csv(\"5_class_back_translated_dataset.csv\")\n",
        "df = df.dropna(subset=[\"back_translated_content\"])\n",
        "df[\"content_cleaned\"] = df[\"back_translated_content\"].astype(str).apply(clean_for_ml)\n",
        "df = df[df[\"content_cleaned\"].str.strip() != \"\"]\n",
        "\n",
        "X = df[\"content_cleaned\"]\n",
        "y_str = df[\"tag\"]\n",
        "\n",
        "y = label_encoder.transform(y_str)\n",
        "\n",
        "# facem split ca mai sus\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Running predictions...\")\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "def plot_conf_matrix(y_true, y_pred, model_name, labels):\n",
        "    \"\"\"Helper function to plot a confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "    plot_obj = disp.plot(cmap=\"OrRd\", xticks_rotation=45)\n",
        "    plot_obj.ax_.grid(False)\n",
        "    plot_obj.ax_.set_title(f\"Confusion Matrix - {model_name}\", pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_conf_matrix(y_test, y_pred, \"SVC back-translated set\", label_encoder.classes_)\n"
      ],
      "metadata": {
        "id": "QtNpWcobMXBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning bert\n",
        "1. Experimente cu Optuna\n",
        "2. BERT final\n",
        "3. Testare bert pe setul tradus"
      ],
      "metadata": {
        "id": "93uJSwwxOn8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers[torch] datasets scikit-learn pandas matplotlib optuna --quiet\n",
        "\n",
        "import optuna\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        "    BertForSequenceClassification\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_file = open(f\"training_log_{timestamp}.txt\", \"w\")\n",
        "sys.stdout = log_file\n",
        "sys.stderr = log_file\n",
        "\n",
        "# seeduri reproductibilitate\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# apelam la gpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# incarcam si preprocesam\n",
        "df = pd.read_csv(\"augmented_fake_rom_combined_correctly_split.csv\")\n",
        "df = df.dropna(subset=[\"content\", \"tag\"])\n",
        "df[\"content\"] = df[\"content\"].astype(str)\n",
        "\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"tag\"])\n",
        "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"Label mapping:\", label_mapping)\n",
        "\n",
        "# afisam distributia claselor\n",
        "for label, count in df[\"tag\"].value_counts().items():\n",
        "    print(f\"{label}: {count} entries\")\n",
        "\n",
        "df = df[[\"content\", \"label\"]]\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "MODEL_CHECKPOINT = \"dumitrescustefan/bert-base-romanian-cased-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"content\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "print(type(dataset[0][\"content\"]), dataset[0][\"content\"])\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# splittuim datele\n",
        "split_1 = tokenized_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
        "train_dataset = split_1[\"train\"]\n",
        "temp_dataset = split_1[\"test\"]\n",
        "split_2 = temp_dataset.train_test_split(test_size=0.5, seed=SEED)\n",
        "val_dataset = split_2[\"train\"]\n",
        "test_dataset = split_2[\"test\"]\n",
        "\n",
        "# functia de metrici\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"f1_weighted\": f1_score(labels, predictions, average=\"weighted\"),\n",
        "        \"f1_macro\": f1_score(labels, predictions, average=\"macro\"),\n",
        "    }\n",
        "\n",
        "def model_init():\n",
        "    return BertForSequenceClassification.from_pretrained(\n",
        "        MODEL_CHECKPOINT,\n",
        "        num_labels=len(label_mapping)\n",
        "    )\n",
        "\n",
        "# definim spatiul de hiperparam\n",
        "def optuna_hp_space(trial: optuna.Trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
        "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 6),\n",
        "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.01, 0.3),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16]),\n",
        "        \"warmup_steps\": trial.suggest_int(\"warmup_steps\", 0, 500),\n",
        "    }\n",
        "\n",
        "# rulam cautarea\n",
        "hpo_training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_results_hpo\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to='none',\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "\n",
        "hpo_trainer = Trainer(\n",
        "    args=hpo_training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    model_init=model_init,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        ")\n",
        "\n",
        "\n",
        "print(\"--- Starting Rigorous Hyperparameter Search ---\")\n",
        "try:\n",
        "    best_run = hpo_trainer.hyperparameter_search(\n",
        "        direction=\"maximize\",\n",
        "        backend=\"optuna\",\n",
        "        hp_space=optuna_hp_space,\n",
        "        n_trials=7,\n",
        "        compute_objective=lambda metrics: metrics[\"eval_accuracy\"],\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Hyperparameter Search Complete ---\")\n",
        "    print(f\"Best objective value (Accuracy): {best_run.objective}\")\n",
        "    print(f\"Best hyperparameters: {best_run.hyperparameters}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during hyperparameter search: {e}\")\n",
        "    best_run = type('BestRun', (), {\n",
        "        'objective': 0.0,\n",
        "        'hyperparameters': {\n",
        "            'learning_rate': 2e-5, 'num_train_epochs': 4, 'weight_decay': 0.01,\n",
        "            'per_device_train_batch_size': 16, 'warmup_steps': 100\n",
        "        }\n",
        "    })()\n",
        "    print(\"Using default hyperparameters due to search failure.\")\n",
        "\n",
        "\n",
        "# antrenam modelul final\n",
        "print(\"\\n--- Training final model with best hyperparameters ---\")\n",
        "\n",
        "final_training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_results_final\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to='none',\n",
        "    metric_for_best_model=\"accuracy\", #verificam acuratetea\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,\n",
        "    seed=SEED,\n",
        "    **best_run.hyperparameters\n",
        ")\n",
        "\n",
        "callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "\n",
        "# folosim trainer in loc de custom trainer\n",
        "final_trainer = Trainer(\n",
        "    model=model_init(),\n",
        "    args=final_training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"Training final model...\")\n",
        "train_result = final_trainer.train()\n",
        "\n",
        "# evaluare\n",
        "print(\"\\n--- Evaluating final model on test set ---\")\n",
        "test_metrics = final_trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
        "print(\"Final Test set metrics:\", test_metrics)\n",
        "\n",
        "# Generate predictions\n",
        "predictions = final_trainer.predict(test_dataset)\n",
        "y_true = predictions.label_ids\n",
        "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "print(\"\\nFinal Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=list(label_mapping.keys())))\n",
        "\n",
        "# plot metrici\n",
        "logs = final_trainer.state.log_history\n",
        "train_loss = [x['loss'] for x in logs if 'loss' in x]\n",
        "eval_loss = [x['eval_loss'] for x in logs if 'eval_loss' in x]\n",
        "accuracy = [x['eval_accuracy'] for x in logs if 'eval_accuracy' in x]\n",
        "f1_macro = [x['eval_f1_macro'] for x in logs if 'eval_f1_macro' in x]\n",
        "f1_weighted = [x['eval_f1_weighted'] for x in logs if 'eval_f1_weighted' in x]\n",
        "epochs = list(range(1, len(accuracy) + 1))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_loss, label=\"Train Loss\")\n",
        "plt.plot(eval_loss, label=\"Eval Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Evaluation Loss\")\n",
        "plt.legend()\n",
        "plt.savefig(\"loss_curve.png\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, accuracy, label=\"Accuracy\")\n",
        "plt.plot(epochs, f1_macro, label=\"F1 Macro\")\n",
        "plt.plot(epochs, f1_weighted, label=\"F1 Weighted\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Evaluation Metrics over Epochs\")\n",
        "plt.legend()\n",
        "plt.savefig(\"metrics_curve.png\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(label_mapping.keys()))\n",
        "disp_plot = disp.plot(xticks_rotation=45, cmap='Blues')\n",
        "disp_plot.ax_.grid(False)\n",
        "disp_plot.ax_.set_title(\"Confusion Matrix - Final Test Set\", pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"confusion_matrix.png\")\n",
        "\n",
        "# salvam modelul la final\n",
        "print(\"\\nSaving final model...\")\n",
        "final_trainer.save_model(\"./fine_tuned_bert_optimized\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bert_optimized\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "sys.stdout = sys.__stdout__\n",
        "sys.stderr = sys.__stderr__\n",
        "log_file.close()\n",
        "\n",
        "print(\"Training complete! Log saved.\")"
      ],
      "metadata": {
        "id": "aBkX6k79OvWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers[torch] datasets scikit-learn pandas matplotlib optuna --quiet\n",
        "\n",
        "import optuna\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        "    BertForSequenceClassification\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Setup Logging ---\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_file = open(f\"training_log_{timestamp}.txt\", \"w\")\n",
        "sys.stdout = log_file\n",
        "sys.stderr = log_file\n",
        "\n",
        "# --- Reproducibility ---\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# --- Basic Setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "df = pd.read_csv(\"augmented_fake_rom_combined_correctly_split.csv\")\n",
        "df = df.dropna(subset=[\"content\", \"tag\"])\n",
        "df[\"content\"] = df[\"content\"].astype(str)\n",
        "\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"tag\"])\n",
        "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"Label mapping:\", label_mapping)\n",
        "\n",
        "# Print class distribution\n",
        "for label, count in df[\"tag\"].value_counts().items():\n",
        "    print(f\"{label}: {count} entries\")\n",
        "\n",
        "df = df[[\"content\", \"label\"]]\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "MODEL_CHECKPOINT = \"dumitrescustefan/bert-base-romanian-cased-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"content\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "print(type(dataset[0][\"content\"]), dataset[0][\"content\"])\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "#splittuim datele\n",
        "split_1 = tokenized_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
        "train_dataset = split_1[\"train\"]\n",
        "temp_dataset = split_1[\"test\"]\n",
        "split_2 = temp_dataset.train_test_split(test_size=0.5, seed=SEED)\n",
        "val_dataset = split_2[\"train\"]\n",
        "test_dataset = split_2[\"test\"]\n",
        "\n",
        "#greutati\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(df[\"label\"]),\n",
        "    y=df[\"label\"]\n",
        ")\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "print(\"Class Weights:\", class_weights_tensor)\n",
        "\n",
        "# metrici\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"f1_weighted\": f1_score(labels, predictions, average=\"weighted\"),\n",
        "        \"f1_macro\": f1_score(labels, predictions, average=\"macro\"),\n",
        "    }\n",
        "\n",
        "# custom trainer sa includa greutatile\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, class_weights=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "#initializam modelul\n",
        "def model_init():\n",
        "    return BertForSequenceClassification.from_pretrained(\n",
        "        MODEL_CHECKPOINT,\n",
        "        num_labels=len(label_mapping)\n",
        "    )\n",
        "\n",
        "#definim spatiul de hiperparametrii\n",
        "def optuna_hp_space(trial: optuna.Trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
        "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 6),\n",
        "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.01, 0.3),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16]),\n",
        "        \"warmup_steps\": trial.suggest_int(\"warmup_steps\", 0, 500),\n",
        "    }\n",
        "\n",
        "# rulam cautaea\n",
        "hpo_training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_results_hpo\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to='none',\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "hpo_trainer = CustomTrainer(\n",
        "    class_weights=class_weights_tensor,\n",
        "    args=hpo_training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    model_init=model_init,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        ")\n",
        "\n",
        "print(\"--- Starting Rigorous Hyperparameter Search (Objective: Maximize F1-Macro) ---\")\n",
        "try:\n",
        "    best_run = hpo_trainer.hyperparameter_search(\n",
        "        direction=\"maximize\",\n",
        "        backend=\"optuna\",\n",
        "        hp_space=optuna_hp_space,\n",
        "        n_trials=10,\n",
        "        compute_objective=lambda metrics: metrics[\"eval_f1_macro\"],\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Hyperparameter Search Complete ---\")\n",
        "    print(f\"Best objective value (F1-Macro): {best_run.objective}\")\n",
        "    print(f\"Best hyperparameters: {best_run.hyperparameters}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during hyperparameter search: {e}\")\n",
        "    best_run = type('BestRun', (), {\n",
        "        'objective': 0.0,\n",
        "        'hyperparameters': {\n",
        "            'learning_rate': 2e-5, 'num_train_epochs': 4, 'weight_decay': 0.01,\n",
        "            'per_device_train_batch_size': 16, 'warmup_steps': 100\n",
        "        }\n",
        "    })()\n",
        "    print(\"Using default hyperparameters due to search failure.\")\n",
        "\n",
        "#antrenam la final cu hiperparametrii buni\n",
        "print(\"\\n--- Training final model with best hyperparameters ---\")\n",
        "\n",
        "final_training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_results_final\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to='none',\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,\n",
        "    seed=SEED,\n",
        "    **best_run.hyperparameters\n",
        ")\n",
        "\n",
        "callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "\n",
        "final_trainer = CustomTrainer(\n",
        "    class_weights=class_weights_tensor,\n",
        "    model=model_init(),\n",
        "    args=final_training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"Training final model...\")\n",
        "train_result = final_trainer.train()\n",
        "#evaluam\n",
        "print(\"\\n--- Evaluating final model on test set ---\")\n",
        "test_metrics = final_trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
        "print(\"Final Test set metrics:\", test_metrics)\n",
        "\n",
        "#generam predictiile\n",
        "predictions = final_trainer.predict(test_dataset)\n",
        "y_true = predictions.label_ids\n",
        "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "print(\"\\nFinal Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=list(label_mapping.keys())))\n",
        "\n",
        "# plot metrici\n",
        "logs = final_trainer.state.log_history\n",
        "train_loss = [x['loss'] for x in logs if 'loss' in x]\n",
        "eval_loss = [x['eval_loss'] for x in logs if 'eval_loss' in x]\n",
        "accuracy = [x['eval_accuracy'] for x in logs if 'eval_accuracy' in x]\n",
        "f1_macro = [x['eval_f1_macro'] for x in logs if 'eval_f1_macro' in x]\n",
        "f1_weighted = [x['eval_f1_weighted'] for x in logs if 'eval_f1_weighted' in x]\n",
        "epochs = list(range(1, len(accuracy) + 1))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_loss, label=\"Train Loss\")\n",
        "plt.plot(eval_loss, label=\"Eval Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Evaluation Loss\")\n",
        "plt.legend()\n",
        "plt.savefig(\"loss_curve.png\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, accuracy, label=\"Accuracy\")\n",
        "plt.plot(epochs, f1_macro, label=\"F1 Macro\")\n",
        "plt.plot(epochs, f1_weighted, label=\"F1 Weighted\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Evaluation Metrics over Epochs\")\n",
        "plt.legend()\n",
        "plt.savefig(\"metrics_curve.png\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(label_mapping.keys()))\n",
        "disp_plot = disp.plot(xticks_rotation=45, cmap='Blues')\n",
        "disp_plot.ax_.grid(False)\n",
        "disp_plot.ax_.set_title(\"Confusion Matrix - Final Test Set\", pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"confusion_matrix.png\")\n",
        "\n",
        "# salvam modelul final\n",
        "print(\"\\nSaving final model...\")\n",
        "final_trainer.save_model(\"./fine_tuned_bert_optimized\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bert_optimized\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "sys.stdout = sys.__stdout__\n",
        "sys.stderr = sys.__stderr__\n",
        "log_file.close()\n",
        "\n",
        "print(\"Training complete! Log saved.\")\n"
      ],
      "metadata": {
        "id": "yHEnHV2ROxtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT varianta finala"
      ],
      "metadata": {
        "id": "naBgh74eO8uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        "    BertForSequenceClassification\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_file = open(f\"training_log_{timestamp}.txt\", \"w\")\n",
        "sys.stdout = log_file\n",
        "sys.stderr = log_file\n",
        "\n",
        "# reproductibilitate\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ne conectam la gpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"augmented_fake_rom_combined_correctly_split.csv\")\n",
        "df = df.dropna(subset=[\"content\", \"tag\"])\n",
        "df[\"content\"] = df[\"content\"].astype(str)\n",
        "\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"tag\"])\n",
        "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"Label mapping:\", label_mapping)\n",
        "\n",
        "# afisam distributia claselor\n",
        "for label, count in df[\"tag\"].value_counts().items():\n",
        "    print(f\"{label}: {count} entries\")\n",
        "\n",
        "df = df[[\"content\", \"label\"]]\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "MODEL_CHECKPOINT = \"dumitrescustefan/bert-base-romanian-cased-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"content\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "print(type(dataset[0][\"content\"]), dataset[0][\"content\"])\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# splittuim datele\n",
        "split_1 = tokenized_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
        "train_dataset = split_1[\"train\"]\n",
        "temp_dataset = split_1[\"test\"]\n",
        "split_2 = temp_dataset.train_test_split(test_size=0.5, seed=SEED)\n",
        "val_dataset = split_2[\"train\"]\n",
        "test_dataset = split_2[\"test\"]\n",
        "\n",
        "# greutati\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(df[\"label\"]),\n",
        "    y=df[\"label\"]\n",
        ")\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "print(\"Class Weights:\", class_weights_tensor)\n",
        "\n",
        "# metricile verificate\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"f1_weighted\": f1_score(labels, predictions, average=\"weighted\"),\n",
        "        \"f1_macro\": f1_score(labels, predictions, average=\"macro\"),\n",
        "    }\n",
        "\n",
        "#clasa custim pt greutati\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, class_weights=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def model_init():\n",
        "    return BertForSequenceClassification.from_pretrained(\n",
        "        MODEL_CHECKPOINT,\n",
        "        num_labels=len(label_mapping)\n",
        "    )\n",
        "\n",
        "#definim hiperparametrii\n",
        "hyperparameters= {\n",
        "    'learning_rate': 4.021289548621635e-05,\n",
        "    'num_train_epochs': 4,\n",
        "    'weight_decay': 0.27790532029550763,\n",
        "    'per_device_train_batch_size': 16,\n",
        "    'warmup_steps': 427\n",
        "}\n",
        "print(f\"Using fixed hyperparameters: {hyperparameters}\")\n",
        "\n",
        "\n",
        "# antrenam modelul final\n",
        "print(\"\\n--- Training model with specified hyperparameters ---\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_results_final\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to='none',\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,\n",
        "    seed=SEED,\n",
        "    **hyperparameters\n",
        ")\n",
        "\n",
        "callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    class_weights=class_weights_tensor,\n",
        "    model=model_init(),\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# evaluam\n",
        "print(\"\\n--- Evaluating final model on test set ---\")\n",
        "test_metrics = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
        "print(\"Final Test set metrics:\", test_metrics)\n",
        "\n",
        "#generam predictiile\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_true = predictions.label_ids\n",
        "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "\n",
        "final_report_str = classification_report(y_true, y_pred, target_names=list(label_mapping.keys()))\n",
        "print(\"\\nFinal Classification Report:\")\n",
        "print(final_report_str)\n",
        "\n",
        "\n",
        "# Plot training metrics\n",
        "logs = trainer.state.log_history\n",
        "train_loss = [x['loss'] for x in logs if 'loss' in x]\n",
        "eval_loss = [x['eval_loss'] for x in logs if 'eval_loss' in x]\n",
        "accuracy = [x['eval_accuracy'] for x in logs if 'eval_accuracy' in x]\n",
        "f1_macro = [x['eval_f1_macro'] for x in logs if 'eval_f1_macro' in x]\n",
        "f1_weighted = [x['eval_f1_weighted'] for x in logs if 'eval_f1_weighted' in x]\n",
        "epochs = list(range(1, len(accuracy) + 1))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_loss, label=\"Train Loss\")\n",
        "plt.plot(eval_loss, label=\"Eval Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Evaluation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"loss_curve.png\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, accuracy, label=\"Accuracy\")\n",
        "plt.plot(epochs, f1_macro, label=\"F1 Macro\")\n",
        "plt.plot(epochs, f1_weighted, label=\"F1 Weighted\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Evaluation Metrics over Epochs\")\n",
        "plt.legend()\n",
        "plt.savefig(\"metrics_curve.png\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(label_mapping.keys()))\n",
        "disp_plot = disp.plot(xticks_rotation=45, cmap='Blues')\n",
        "disp_plot.ax_.grid(False)\n",
        "disp_plot.ax_.set_title(\"Confusion Matrix - Final Test Set\", pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"confusion_matrix.png\")\n",
        "\n",
        "# salvam modelul la final\n",
        "print(\"\\nSaving final model...\")\n",
        "trainer.save_model(\"./fine_tuned_bert_optimized\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bert_optimized\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "\n",
        "sys.stdout = sys.__stdout__\n",
        "sys.stderr = sys.__stderr__\n",
        "log_file.close()\n",
        "\n",
        "print(\"Training complete! Log saved to file.\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Final Classification Report (Test Set)\")\n",
        "print(\"=\"*50)\n",
        "print(final_report_str)"
      ],
      "metadata": {
        "id": "3-2470V-O0Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testare BERT pe setul tradus"
      ],
      "metadata": {
        "id": "tTPQF9jDPASS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "#!pip install transformers datasets scikit-learn matplotlib\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"5_class_back_translated_dataset.csv\")\n",
        "df = df.dropna(subset=[\"content\", \"tag\"])\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"/content/fine_tuned_bert_optimized\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"/content/fine_tuned_bert_optimized\")\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"back_translated_content\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.class_encode_column(\"tag\")\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"tag\", \"label\")\n",
        "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "\n",
        "# stratificam setul ca in cealalta situatie\n",
        "\n",
        "split_1 = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = split_1[\"train\"]\n",
        "temp_dataset = split_1[\"test\"]\n",
        "split_2 = temp_dataset.train_test_split(test_size=0.5, seed=42)\n",
        "val_dataset = split_2[\"train\"]\n",
        "test_dataset = split_2[\"test\"]\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Predict\n",
        "predictions = trainer.predict(test_dataset)\n",
        "pred_labels = predictions.predictions.argmax(-1)\n",
        "true_labels = predictions.label_ids\n",
        "# metrici\n",
        "print(\"Classification Report:\\n\", classification_report(true_labels, pred_labels))\n",
        "\n",
        "label_names = [\"fake_news\", \"misinformation\", \"propaganda\", \"real_news\", \"satire\"]\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)\n",
        "disp.plot(cmap='OrRd', xticks_rotation=45)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sa8jQjhMO7gb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}